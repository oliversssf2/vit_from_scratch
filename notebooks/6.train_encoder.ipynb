{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an classifier with a transformer encoder \n",
    "In this notebook, we'll train the transformer encoder with the imdb dataset. Imdb is a movie review dataset for binary sentiment classification in which we have to classify whether a text reivew is negative (0) or positive (1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/vit/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1130.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = datasets.load_dataset('imdb')\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}\n",
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0} \n",
      "\n",
      "{'text': \"...let me count the ways.<br /><br />1. A title-only 'remake' that pulls out every cliché in the slasher handbook.<br /><br />2. A plot so predictable that it becomes quite pathetic.<br /><br />3. A completely weak execution of all attempts at suspense or thrills.<br /><br />4. A PG-13 rating that insures no gore, violence, or sex.<br /><br />5. A villain that is not frightening or even mysterious.<br /><br />6. A cast of characters that are so thinly written and stereotyped that we couldn't possibly care about them.<br /><br />7. A lack of any effectively creepy atmosphere (much unlike the original Prom Night).<br /><br />8. A script of dialog that's beyond poor - it's mind numbing. <br /><br />9. A series of cardboard performances (not sure whether to blame the actors or the lousy aforementioned script for that).<br /><br />10. A completely inept teen-targeted slasher remake that's not brave enough to attempt to have an imagination - or even to show a puddle of blood.<br /><br />It's a no-brainer horror fans, save your money.<br /><br />BOMB out of ****\", 'label': 0} \n",
      "\n",
      "{'text': 'The movie was much better than the other reviewer stated. It\\'s a nice family movie. It has a fun fantasy aspect of some time travel. The story revolves around a 14 year old girl who accidentally finds a way to travel back in time in the old elevator of her apartment building. Of course, no one believes her when she tries to explain her disappearances. She finds and makes friends with a girl about her age and is able to help the girl\\'s family in many ways. She is also able to help her own relationship with her father in the long run. It reminds me of a Hallmark movie so give it a chance and decide for yourself. It seemed to be aimed more towards children about 6-12 years (maybe a bit older) and it\\'s pretty much PG or G rated. I\\'m an adult who can appreciate a nice \"family\" movie - I guess the other reviewer isn\\'t.', 'label': 1} \n",
      "\n",
      "{'text': \"Saw this movie when it came out in 1959, left a lasting impression. Great group of actors. A little short timewise but a great movie all the same. Have only seen once since then and that was some time ago. Hopefully they'll put it out on DVD if they haven't already.\", 'label': 1} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below are a summary and several examples of the imdb dataset\n",
    "from pprint import pprint\n",
    "\n",
    "print(imdb['train'].features)\n",
    "print(imdb['train'][0], '\\n')\n",
    "print(imdb['train'][734], '\\n')\n",
    "print(imdb['train'][19375], '\\n')\n",
    "print(imdb['train'][20075], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We will tokenizer the entire dataset during preprocessing, then batch them and create the labels on the fly in the collate function of the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we'll download a pretrained tokenizer from huggingface\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 11.77ba/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 12.33ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization with the `batch_encode_plus` function\n",
    "imdb['train'] = imdb['train'].map(\n",
    "    lambda x: tokenizer.batch_encode_plus(\n",
    "        x['text'], padding=False, return_attention_mask=False, truncation=True), \n",
    "    batched=True)\n",
    "imdb['test'] = imdb['test'].map(\n",
    "    lambda x: tokenizer.batch_encode_plus(\n",
    "        x['text'], padding=False, return_attention_mask=False, truncation=True), \n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0, 'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]}\n"
     ]
    }
   ],
   "source": [
    "print(imdb['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_batch = imdb['train'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this padding function to pad the input_ids to the same length\n",
    "output = tokenizer.pad(dummy_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collate_fn is run on each data batch, padding and \n",
    "# transforming every data batch to pytorch tensors\n",
    "import torch\n",
    "def collate_fn(batch):\n",
    "    batch = tokenizer.pad(batch)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(batch['input_ids']),\n",
    "        'attn_mask': torch.tensor(batch['attention_mask']),\n",
    "        'labels': torch.tensor(batch['label'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we create dataloaders for the train and test splits\n",
    "# of the dataset for us to iterate over during training\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    imdb['train'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2065,  2017,  ...,     0,     0,     0],\n",
       "         [  101,  2074, 14395,  ...,     0,     0,     0],\n",
       "         [  101,  2026, 15003,  ...,  3272,  2077,   102],\n",
       "         [  101,  2023,  2265,  ...,     0,     0,     0]]),\n",
       " 'attn_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 0, 0, 0])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(train_dataloader)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now try to feed the batched data into our model\n",
    "\n",
    "from vit.encoder import TransformerEncoderClassifer\n",
    "\n",
    "classifier = TransformerEncoderClassifer(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    num_layer=6,\n",
    "    num_head=8,\n",
    "    d_k=8,\n",
    "    dropout_rate=0.1,\n",
    "    num_class=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick whichever device you want!\n",
    "device='cuda:0'\n",
    "# device='cpu'\n",
    "\n",
    "train_batch = next(it)\n",
    "classifier.to(device)\n",
    "output = classifier.forward(\n",
    "    x=train_batch['input_ids'].to(device),\n",
    "    attn_mask=train_batch['attn_mask'].to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2218, -0.1533],\n",
       "        [ 0.1998, -0.1332],\n",
       "        [ 0.0444, -0.1459],\n",
       "        [-0.0451, -0.1249]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the Cross Entropy Loss between the output and labels\n",
    "loss = torch.nn.CrossEntropyLoss()(output, train_batch['labels'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make sure the gradient flows through the entire classifier\n",
    "loss.backward()\n",
    "for name, param in classifier.named_parameters():\n",
    "    assert param.grad is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 970.98it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-e62e61132651b481.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-a39490da13cfcc47.arrow\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from vit.encoder import TransformerEncoderClassifer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import datasets \n",
    "from tqdm import tqdm # tqdm is a python progress bar library\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Training configurations\n",
    "num_epoch = 100\n",
    "learning_rate = 1e-5\n",
    "optimizer = 'adam'\n",
    "device='cuda:0'\n",
    "\n",
    "# Data configurations\n",
    "batch_size=100\n",
    "\n",
    "# Model configurations\n",
    "vocab_size=tokenizer.vocab_size\n",
    "d_model=256\n",
    "num_layer=2\n",
    "num_head=8\n",
    "d_k=32\n",
    "dropout_rate=0.1\n",
    "num_class=2\n",
    "\n",
    "imdb = datasets.load_dataset('imdb')\n",
    "# imdb\n",
    "\n",
    "# Tokenization with the `batch_encode_plus` function\n",
    "imdb['train'] = imdb['train'].map(\n",
    "    lambda x: tokenizer.batch_encode_plus(\n",
    "        x['text'], padding=False, return_attention_mask=False, truncation=True), \n",
    "    batched=True)\n",
    "imdb['test'] = imdb['test'].map(\n",
    "    lambda x: tokenizer.batch_encode_plus(\n",
    "        x['text'], padding=False, return_attention_mask=False, truncation=True), \n",
    "    batched=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = tokenizer.pad(batch)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(batch['input_ids']),\n",
    "        'attn_mask': torch.tensor(batch['attention_mask']),\n",
    "        'labels': torch.tensor(batch['label'])\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    imdb['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers = 24\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    imdb['test'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers = 24\n",
    ")\n",
    "\n",
    "classifier = TransformerEncoderClassifer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_layer=num_layer,\n",
    "    num_head=num_head,\n",
    "    d_k=d_k,\n",
    "    dropout_rate=dropout_rate,\n",
    "    num_class=num_class\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  33%|███▎      | 82/250 [03:49<07:49,  2.80s/it, Loss=0.694, Accuracy=0.54] \n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epoch):\n",
    "    # Train\n",
    "    with tqdm(train_dataloader) as train_epoch:\n",
    "        for batch_id, batch in enumerate(train_epoch):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attn_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # if batch_id > 3:\n",
    "            #     break\n",
    "\n",
    "            outputs = classifier(\n",
    "                x=input_ids,\n",
    "                attn_mask=attn_mask\n",
    "            )\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "            predictions = outputs.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(loss)\n",
    "            # print(accuracy)\n",
    "            train_epoch.set_description(f\"Training Epoch {i}\")\n",
    "            train_epoch.set_postfix({\n",
    "                'Loss': loss.item(), \n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "    # Validate\n",
    "    with tqdm(test_dataloader) as test_epoch:\n",
    "        for batch_id, batch in enumerate(test_epoch):\n",
    "            # if batch_id > 3:\n",
    "            #     break\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attn_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = classifier(\n",
    "                x=input_ids,\n",
    "                attn_mask=attn_mask\n",
    "            )\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "            predictions = outputs.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "\n",
    "            test_epoch.set_description(f\"Test Epoch {i}\")\n",
    "            test_epoch.set_postfix({\n",
    "                'Loss': loss.item(), \n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afa5cafc83a44ddc6a4432c58e27e1e59e90bf4b5432b3ef6f80dec7803e93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
