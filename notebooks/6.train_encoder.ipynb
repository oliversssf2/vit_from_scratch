{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an classifier with a transformer encoder \n",
    "In this notebook, we'll train the transformer encoder with the imdb dataset. Imdb is a movie review dataset for binary sentiment classification in which we have to classify whether a text reivew is negative (0) or positive (1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fongsu/miniconda3/envs/vit/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/fongsu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 543.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = datasets.load_dataset('imdb')\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0} \n",
      "\n",
      "{'text': \"...let me count the ways.<br /><br />1. A title-only 'remake' that pulls out every cliché in the slasher handbook.<br /><br />2. A plot so predictable that it becomes quite pathetic.<br /><br />3. A completely weak execution of all attempts at suspense or thrills.<br /><br />4. A PG-13 rating that insures no gore, violence, or sex.<br /><br />5. A villain that is not frightening or even mysterious.<br /><br />6. A cast of characters that are so thinly written and stereotyped that we couldn't possibly care about them.<br /><br />7. A lack of any effectively creepy atmosphere (much unlike the original Prom Night).<br /><br />8. A script of dialog that's beyond poor - it's mind numbing. <br /><br />9. A series of cardboard performances (not sure whether to blame the actors or the lousy aforementioned script for that).<br /><br />10. A completely inept teen-targeted slasher remake that's not brave enough to attempt to have an imagination - or even to show a puddle of blood.<br /><br />It's a no-brainer horror fans, save your money.<br /><br />BOMB out of ****\", 'label': 0} \n",
      "\n",
      "{'text': 'The movie was much better than the other reviewer stated. It\\'s a nice family movie. It has a fun fantasy aspect of some time travel. The story revolves around a 14 year old girl who accidentally finds a way to travel back in time in the old elevator of her apartment building. Of course, no one believes her when she tries to explain her disappearances. She finds and makes friends with a girl about her age and is able to help the girl\\'s family in many ways. She is also able to help her own relationship with her father in the long run. It reminds me of a Hallmark movie so give it a chance and decide for yourself. It seemed to be aimed more towards children about 6-12 years (maybe a bit older) and it\\'s pretty much PG or G rated. I\\'m an adult who can appreciate a nice \"family\" movie - I guess the other reviewer isn\\'t.', 'label': 1} \n",
      "\n",
      "{'text': \"Saw this movie when it came out in 1959, left a lasting impression. Great group of actors. A little short timewise but a great movie all the same. Have only seen once since then and that was some time ago. Hopefully they'll put it out on DVD if they haven't already.\", 'label': 1} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below are a summary and several examples of the imdb dataset\n",
    "from pprint import pprint\n",
    "\n",
    "print(imdb['train'].features)\n",
    "print(imdb['train'][0], '\\n')\n",
    "print(imdb['train'][734], '\\n')\n",
    "print(imdb['train'][19375], '\\n')\n",
    "print(imdb['train'][20075], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We will tokenizer the entire dataset during preprocessing, then batch them and create the labels on the fly in the collate function of the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we'll download a pretrained tokenizer from huggingface\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/fongsu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-8e31a14799a51d22.arrow\n",
      "Loading cached processed dataset at /home/fongsu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2ad5fdc294050aaf.arrow\n"
     ]
    }
   ],
   "source": [
    "# Tokenization with the `batch_encode_plus` function\n",
    "imdb['train'] = imdb['train'].map(\n",
    "    lambda x: tokenizer.batch_encode_plus(\n",
    "        x['text'], padding=False, return_attention_mask=False, truncation=True), \n",
    "    batched=True)\n",
    "imdb['test'] = imdb['test'].map(\n",
    "    lambda x: tokenizer.batch_encode_plus(\n",
    "        x['text'], padding=False, return_attention_mask=False, truncation=True), \n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0, 'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]}\n"
     ]
    }
   ],
   "source": [
    "print(imdb['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_batch = imdb['train'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# We use this padding function to pad the input_ids to the same length\n",
    "output = tokenizer.pad(dummy_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collate_fn is run on each data batch, padding and \n",
    "# transforming every data batch to pytorch tensors\n",
    "import torch\n",
    "def collate_fn(batch):\n",
    "    batch = tokenizer.pad(batch)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(batch['input_ids']),\n",
    "        'attn_mask': torch.tensor(batch['attention_mask']),\n",
    "        'labels': torch.tensor(batch['label'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we create dataloaders for the train and test splits\n",
    "# of the dataset for us to iterate over during training\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    imdb['train'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1008,  1008,  ...,     0,     0,     0],\n",
       "         [  101,  1037,  2843,  ..., 10024,  7062,   102],\n",
       "         [  101,  1045,  2064,  ...,     0,     0,     0],\n",
       "         [  101,  2130,  3889,  ...,     0,     0,     0]]),\n",
       " 'attn_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 1, 1, 0])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(train_dataloader)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "# Let's now try to feed the batched data into our model\n",
    "\n",
    "from vit.encoder import TransformerEncoderClassifer\n",
    "\n",
    "classifier = TransformerEncoderClassifer(\n",
    "    # vocab_size=tokenizer.vocab_size,\n",
    "    vocab_size=4,\n",
    "    d_model=512,\n",
    "    num_layer=6,\n",
    "    num_head=8,\n",
    "    d_k=8,\n",
    "    dropout_rate=0.1,\n",
    "    num_class=2,\n",
    "    max_len=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fongsu/repos/vit_from_scratch/vit/positional_encoding.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoding = torch.tensor(self.pos_encodings[:seq_len], device=x.device)\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [205,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m train_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(it)\n\u001b[1;32m      6\u001b[0m classifier\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m output \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m      8\u001b[0m     x\u001b[39m=\u001b[39;49mtrain_batch[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m      9\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mtrain_batch[\u001b[39m'\u001b[39;49m\u001b[39mattn_mask\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/repos/vit_from_scratch/vit/encoder.py:100\u001b[0m, in \u001b[0;36mTransformerEncoderClassifer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, attn_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m     \u001b[39m# x: [batch_size, seq_len] -> [batch_size, seq_len, d_model]\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m    101\u001b[0m     \u001b[39m# Here we use the hidden_state of the first token as the input for\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# classification\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[39m# x: [batch_size, seq_len, d_model] -> pooled_x: [batch_size, d,model]\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     pooled_x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/vit_from_scratch/vit/encoder.py:68\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, attn_mask):\n\u001b[1;32m     66\u001b[0m     \u001b[39m# x: [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> 68\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositional_encoding(x)\n\u001b[1;32m     69\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers(x, attn_mask)\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/vit_from_scratch/vit/positional_encoding.py:26\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# dims = torch.arange(0, d_model, device=x.device)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# positions = torch.arange(0, seq_len, device=x.device)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# encoding = torch.tensor([[self.pos_encode(d, p, d_model) for d in dims] for p in positions], device=x.device)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m encoding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encodings[:seq_len], device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 26\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(encoding)\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x \u001b[39m+\u001b[39m encoding)\n",
      "File \u001b[0;32m~/miniconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniconda3/envs/vit/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Pick whichever device you want!\n",
    "device='cuda:0'\n",
    "# device='cpu'\n",
    "\n",
    "train_batch = next(it)\n",
    "classifier.to(device)\n",
    "output = classifier.forward(\n",
    "    x=train_batch['input_ids'].to(device),\n",
    "    attn_mask=train_batch['attn_mask'].to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2218, -0.1533],\n",
       "        [ 0.1998, -0.1332],\n",
       "        [ 0.0444, -0.1459],\n",
       "        [-0.0451, -0.1249]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the Cross Entropy Loss between the output and labels\n",
    "loss = torch.nn.CrossEntropyLoss()(output, train_batch['labels'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make sure the gradient flows through the entire classifier\n",
    "loss.backward()\n",
    "for name, param in classifier.named_parameters():\n",
    "    assert param.grad is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 970.98it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-e62e61132651b481.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-a39490da13cfcc47.arrow\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from vit.encoder import TransformerEncoderClassifer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import datasets \n",
    "from tqdm import tqdm # tqdm is a python progress bar library\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Training configurations\n",
    "num_epoch = 100\n",
    "learning_rate = 1e-5\n",
    "optimizer = 'adam'\n",
    "device='cuda:0'\n",
    "\n",
    "# Data configurations\n",
    "batch_size=100\n",
    "\n",
    "# Model configurations\n",
    "vocab_size=tokenizer.vocab_size\n",
    "d_model=256\n",
    "num_layer=2\n",
    "num_head=8\n",
    "d_k=32\n",
    "dropout_rate=0.1\n",
    "num_class=2\n",
    "\n",
    "imdb = datasets.load_dataset('imdb')\n",
    "# imdb\n",
    "\n",
    "# Tokenization with the `batch_encode_plus` function\n",
    "imdb['train'] = imdb['train'].map(\n",
    "    lambda x: tokenizer.batch_encode_plus(\n",
    "        x['text'], padding=False, return_attention_mask=False, truncation=True), \n",
    "    batched=True)\n",
    "imdb['test'] = imdb['test'].map(\n",
    "    lambda x: tokenizer.batch_encode_plus(\n",
    "        x['text'], padding=False, return_attention_mask=False, truncation=True), \n",
    "    batched=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = tokenizer.pad(batch)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(batch['input_ids']),\n",
    "        'attn_mask': torch.tensor(batch['attention_mask']),\n",
    "        'labels': torch.tensor(batch['label'])\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    imdb['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers = 24\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    imdb['test'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers = 24\n",
    ")\n",
    "\n",
    "classifier = TransformerEncoderClassifer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_layer=num_layer,\n",
    "    num_head=num_head,\n",
    "    d_k=d_k,\n",
    "    dropout_rate=dropout_rate,\n",
    "    num_class=num_class\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:  33%|███▎      | 82/250 [03:49<07:49,  2.80s/it, Loss=0.694, Accuracy=0.54] \n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epoch):\n",
    "    # Train\n",
    "    with tqdm(train_dataloader) as train_epoch:\n",
    "        for batch_id, batch in enumerate(train_epoch):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attn_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # if batch_id > 3:\n",
    "            #     break\n",
    "\n",
    "            outputs = classifier(\n",
    "                x=input_ids,\n",
    "                attn_mask=attn_mask\n",
    "            )\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "            predictions = outputs.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(loss)\n",
    "            # print(accuracy)\n",
    "            train_epoch.set_description(f\"Training Epoch {i}\")\n",
    "            train_epoch.set_postfix({\n",
    "                'Loss': loss.item(), \n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "    # Validate\n",
    "    with tqdm(test_dataloader) as test_epoch:\n",
    "        for batch_id, batch in enumerate(test_epoch):\n",
    "            # if batch_id > 3:\n",
    "            #     break\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attn_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = classifier(\n",
    "                x=input_ids,\n",
    "                attn_mask=attn_mask\n",
    "            )\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "            predictions = outputs.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "\n",
    "            test_epoch.set_description(f\"Test Epoch {i}\")\n",
    "            test_epoch.set_postfix({\n",
    "                'Loss': loss.item(), \n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13c9cfb0b91effbeff207f29489cb85bdbc572f931a40ffc006da1953c5d00d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
