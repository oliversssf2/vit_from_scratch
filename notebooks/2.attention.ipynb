{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention mechanism and multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having a encoder-decoder framewark in mind, we now build the compoments of a transformer one by one, starting from the attention mechanism (the selected area below). \n",
    "\n",
    "We'll first build a single-head attention attention, then extend it to multi-head and implement masking for decoder inputs\n",
    "\n",
    "![transformer model with its attention mechanism selected](./photos/Screenshot%20from%202022-09-07%2017-36-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [transformer paper](https://arxiv.org/pdf/1706.03762.pdf), the hidden states are first **linearly projected** to the key vector, query vector, and the value vector, then computed according to the formula below:\n",
    "$$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}V)$$\n",
    "\n",
    "![An illustration of the attention mechanism](./photos/Screenshot%20from%202022-09-07%2018-12-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated in the illustration above, the query vector $Q$ and the key vector $K$ has the same dimensions, that is, $d_q = d_k$. The value vector can have any dimensions $d_v$ if we add an addition linear layer before the $SoftMax$ layer of $Q$ and $K$, yet in this paper $d_k = d_q = d_v$.\n",
    "\n",
    "The linear projection matrices for a single-head attention mechanism, $W^Q$, $W^K$, $W^V$,  are of dimensions $d_{model}\\times d_k$, $d_{model}\\times d_q$, $d_{model}\\times d_v$, respectively.\n",
    "\n",
    "Below is an implementation of the single-head attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_k)\n",
    "        self.w_k = nn.Linear(d_model, d_k)\n",
    "        self.w_v = nn.Linear(d_model, d_k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # q,k,v: [batch_size, seq_len, d_k]\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        # scores, normalized_scores, softmax_scores: [batch_size, seq_len, seq_len]\n",
    "        scores = torch.matmul(q, torch.permute(k, (0, 2, 1)))\n",
    "        normalized_scores = scores/self.d_k\n",
    "        softmax_scores = nn.functional.softmax(normalized_scores, dim=-1)\n",
    "        #TODO: verify whether the softmax is applied along the correct dimesion\n",
    "\n",
    "        # attentions: [batch_size, seq_len, d_k]\n",
    "        attentions = torch.matmul(softmax_scores, v)\n",
    "        return {\n",
    "            'k': k,\n",
    "            'q': q,\n",
    "            'v': v,\n",
    "            'softmax_scores': softmax_scores,\n",
    "            'attentions': attentions\n",
    "        }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's write tests to make sure the dimensions and the computed attentions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_head_attention = SingleHeadAttention(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = torch.randn((7,4,5)) # A dummy input of shape [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_outputs = single_head_attention(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dummy_outputs['attentions'].shape == (7,4,3) \n",
    "# Make sure that the shape of the attentions is [batch_size, seq_len, d_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2249,  0.0661,  0.6594],\n",
      "         [-0.5190, -0.0567,  0.1383],\n",
      "         [-0.6636, -0.3275,  1.1567],\n",
      "         [-0.9605, -0.0276,  1.5279]],\n",
      "\n",
      "        [[ 0.1181, -0.2155,  0.0951],\n",
      "         [-0.9050, -0.1242,  0.6470],\n",
      "         [ 0.2617,  0.0027, -0.0351],\n",
      "         [-0.3784,  0.4812, -0.6979]],\n",
      "\n",
      "        [[ 1.4064, -0.0663,  0.2217],\n",
      "         [-0.8405, -1.2527,  0.9365],\n",
      "         [-0.7726, -0.0266,  0.2264],\n",
      "         [-0.6351,  0.4558,  0.3582]],\n",
      "\n",
      "        [[ 0.3821,  0.6742, -0.3431],\n",
      "         [-0.4670,  0.7657, -0.3341],\n",
      "         [-0.4310,  0.0676,  0.6857],\n",
      "         [-0.3748,  1.1992, -0.9629]],\n",
      "\n",
      "        [[ 0.6693, -0.5286,  0.5813],\n",
      "         [-1.4740, -0.1369,  0.1863],\n",
      "         [-0.5995,  0.0515, -0.5938],\n",
      "         [-2.0923, -0.5931,  1.2989]],\n",
      "\n",
      "        [[-0.8034,  0.2576,  0.0963],\n",
      "         [-0.8780, -0.1405, -0.2514],\n",
      "         [ 0.3408,  0.7539, -1.0165],\n",
      "         [-0.5890, -0.3180,  0.8081]],\n",
      "\n",
      "        [[ 0.1114, -0.1259, -0.3868],\n",
      "         [ 0.8805,  0.5552, -1.2451],\n",
      "         [-1.0061,  0.4396,  0.7252],\n",
      "         [ 0.3046,  1.0994, -1.1750]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-2.2564e-01, -4.6518e-02,  3.5683e-01],\n",
      "         [ 3.8213e-01, -2.9310e-02, -1.5548e-01],\n",
      "         [ 1.2406e-01, -1.3212e-03, -3.3041e-02],\n",
      "         [ 3.3859e-01, -2.1466e-02, -3.4509e-02]],\n",
      "\n",
      "        [[ 3.1034e-02, -1.5439e-02,  9.2338e-03],\n",
      "         [ 5.7654e-02, -4.5861e-02,  4.4471e-02],\n",
      "         [ 1.6532e-02, -6.3488e-02,  9.5611e-02],\n",
      "         [ 6.4162e-02, -1.1006e-01,  1.4347e-01]],\n",
      "\n",
      "        [[-3.3375e-01,  1.5453e-01, -1.6573e-01],\n",
      "         [-3.7990e-02, -1.6619e-01, -5.0614e-01],\n",
      "         [-1.3905e-01, -1.1552e-01, -5.8595e-01],\n",
      "         [-1.9572e-01, -1.8834e-02, -3.8640e-02]],\n",
      "\n",
      "        [[-6.4488e-02, -1.4941e-01,  2.3575e-01],\n",
      "         [ 7.3794e-02, -6.5877e-01,  4.8920e-01],\n",
      "         [ 6.0747e-02, -9.1846e-01,  6.7447e-01],\n",
      "         [ 9.2226e-02, -6.0901e-01,  4.3104e-01]],\n",
      "\n",
      "        [[ 2.4474e-01,  5.3224e-02, -1.3768e-02],\n",
      "         [-7.4548e-01, -1.6864e-01,  1.6412e-01],\n",
      "         [-6.1091e-01, -9.3508e-02,  3.4858e-03],\n",
      "         [ 1.1613e-01,  9.2870e-02, -3.0260e-01]],\n",
      "\n",
      "        [[ 9.8577e-02,  8.7831e-02, -1.8025e-01],\n",
      "         [ 2.5047e-02,  1.1757e-01, -1.9729e-01],\n",
      "         [ 5.2764e-02, -1.7836e-01,  1.8899e-01],\n",
      "         [-7.1425e-02, -5.4250e-05,  4.6721e-02]],\n",
      "\n",
      "        [[-2.3524e-01, -7.7312e-01,  8.6228e-01],\n",
      "         [-4.7111e-01, -6.9864e-01,  1.0561e+00],\n",
      "         [-2.6179e-01,  3.3734e-01,  7.6173e-03],\n",
      "         [-6.1587e-01, -1.6230e+00,  1.9659e+00]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(dummy_outputs['v'])\n",
    "print(dummy_outputs['attentions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-headed attentions\n",
    "With this simple single-headed attention module, we can now try to implement a multi-headed one based on it. The multi-headed attention mechanism would require us to expand the dimensions of the q,k,v vectors, and include a concatenation and an additional concatenation layer after the attention computations. (As shown in the illustration below)\n",
    "\n",
    "![single-headed and multi-headed attention mechanism](./photos/Screenshot%20from%202022-09-07%2021-19-42.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_head, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.num_head = num_head\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # The output matrices from these linear layers would be \n",
    "        # slice into num_head parts to preform multi-headed \n",
    "        # attention calculation\n",
    "        self.w_q = nn.Linear(d_model, d_k*num_head)\n",
    "        self.w_k = nn.Linear(d_model, d_k*num_head)\n",
    "        self.w_v = nn.Linear(d_model, d_k*num_head)\n",
    "        self.final_linear = nn.Linear(d_k*num_head, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "\n",
    "        # q,k,v: [batch_size, seq_len, num_head*d_k]\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        # scores, normalized_scores, softmax_scores: [batch_size, seq_len, seq_len]\n",
    "\n",
    "        # q: [batch_size, seq_len, num_head*d_k] -> [batch_size, num_head, seq_len, d_k]\n",
    "        q = torch.permute(q, (0, 2, 1)).reshape(batch_size, self.num_head, self.d_k, \n",
    "            seq_len).permute(0, 1, 3, 2)\n",
    "        assert q.shape == (batch_size, self.num_head, seq_len, self.d_k)\n",
    "\n",
    "        # k: [batch_size, seq_len, num_head*d_k] -> [batch_size, num_head, d_k, seq_len]\n",
    "        k = torch.permute(k, (0, 2, 1)).reshape(batch_size, self.num_head, self.d_k, \n",
    "            seq_len)\n",
    "        assert k.shape == (batch_size, self.num_head, self.d_k, seq_len)\n",
    "        \n",
    "        # scores, normalized_scores: [batch_size, num_head, seq_len, seq_len]\n",
    "        scores = torch.matmul(q, k)\n",
    "        assert scores.shape == (batch_size, self.num_head, seq_len, seq_len)\n",
    "    \n",
    "        normalized_scores = scores/self.d_k\n",
    "        softmax_scores = nn.functional.softmax(normalized_scores, dim=-1)\n",
    "        #TODO: verify whether the softmax is applied along the correct dimesion\n",
    "\n",
    "        # v: [batch_size, seq_len, num_head*d_k] -> [batch_size, num_head, seq_len, d_k]\n",
    "        v = torch.permute(v, (0, 2, 1)).reshape(batch_size, self.num_head, self.d_k, \n",
    "            seq_len).permute(0, 1, 3, 2)\n",
    "\n",
    "        assert v.shape == (batch_size, self.num_head, seq_len, self.d_k)\n",
    "\n",
    "        # attentions: [batch_size, num_head, seq_len, d_k]\n",
    "        attentions = torch.matmul(normalized_scores, v)\n",
    "\n",
    "        # Concat attentions from all the attention heads\n",
    "        # attentions: [batch_size, num_head, seq_len, d_k] ->\n",
    "        # [batch_size, seq_len, num_head*d_k]\n",
    "        attentions = attentions.permute(0,2,1,3).reshape((batch_size, seq_len, self.num_head*self.d_k))\n",
    "        assert attentions.shape == (batch_size, seq_len, self.num_head*self.d_k)\n",
    "\n",
    "        # Final linear layer: \n",
    "        #[batch_size, seq_len, num_head*d_k] -> [batch_size, seq_len, d_model]\n",
    "        outputs = self.final_linear(attentions)\n",
    "        assert outputs.shape == (batch_size, seq_len, self.d_model)\n",
    "\n",
    "        return {\n",
    "            'k': k,\n",
    "            'q': q,\n",
    "            'v': v,\n",
    "            'softmax_scores': softmax_scores,\n",
    "            'attentions': attentions,\n",
    "            'outputs': outputs\n",
    "        }\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention = MultiHeadAttention(2, 3, 5).to('cuda:0') #[d_model, num_head, d_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = torch.randn((7, 11, 2)).to('cuda:0') #[batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 11, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batch_size, seq_len, d_model] == (7, 11, 2)\n",
    "multihead_attention(dummy_inputs)['outputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8120,  0.3692],\n",
       "         [-0.7812,  0.5577],\n",
       "         [-0.4525, -0.2322],\n",
       "         [-0.8219,  0.9419],\n",
       "         [-0.6648, -0.2185],\n",
       "         [-0.1780, -0.7772],\n",
       "         [-0.6171,  0.1114],\n",
       "         [-0.5702, -0.4131],\n",
       "         [-0.9026,  0.9138],\n",
       "         [-1.0254,  1.0927],\n",
       "         [ 0.0157, -1.1121]],\n",
       "\n",
       "        [[-0.3096, -0.3299],\n",
       "         [-1.9933,  1.7937],\n",
       "         [ 0.0325, -0.6157],\n",
       "         [ 0.4420, -1.3144],\n",
       "         [ 0.0763, -0.8618],\n",
       "         [-0.7081,  0.4242],\n",
       "         [-2.1309,  2.0334],\n",
       "         [-1.6788,  1.1860],\n",
       "         [-0.8109,  0.2756],\n",
       "         [-1.1934,  0.8401],\n",
       "         [-1.4569,  1.0500]],\n",
       "\n",
       "        [[-2.2969,  1.9327],\n",
       "         [-1.1403,  0.7162],\n",
       "         [-0.6666,  0.1410],\n",
       "         [ 1.0163, -1.6569],\n",
       "         [-0.1207, -0.4114],\n",
       "         [-2.3476,  2.0281],\n",
       "         [-0.7486,  0.3493],\n",
       "         [-0.7244,  0.1886],\n",
       "         [-1.6772,  1.2432],\n",
       "         [-1.6050,  1.1225],\n",
       "         [ 0.3330, -0.9215]],\n",
       "\n",
       "        [[ 1.2644, -1.5887],\n",
       "         [-1.0779,  0.7193],\n",
       "         [ 0.0429, -0.3844],\n",
       "         [-0.5973,  0.2578],\n",
       "         [-0.4319,  0.1166],\n",
       "         [ 0.2102, -0.5437],\n",
       "         [-0.3524,  0.0141],\n",
       "         [ 0.1141, -0.4673],\n",
       "         [-0.4457,  0.1086],\n",
       "         [-0.6076,  0.2612],\n",
       "         [ 0.0802, -0.3924]],\n",
       "\n",
       "        [[-1.0443,  0.6817],\n",
       "         [-0.7068,  0.3025],\n",
       "         [-1.3993,  0.8836],\n",
       "         [-0.7036,  0.2698],\n",
       "         [-0.2435, -0.2442],\n",
       "         [-0.8363,  0.4672],\n",
       "         [-0.9259,  0.4140],\n",
       "         [-0.5619,  0.2017],\n",
       "         [-1.0220,  0.4775],\n",
       "         [-0.6276,  0.2825],\n",
       "         [-0.6349,  0.1280]],\n",
       "\n",
       "        [[-0.7160,  0.3005],\n",
       "         [-0.1756, -0.2288],\n",
       "         [-1.5060,  1.1694],\n",
       "         [-0.2737, -0.1646],\n",
       "         [ 1.1911, -1.5239],\n",
       "         [-0.0240, -0.3695],\n",
       "         [-0.9325,  0.6159],\n",
       "         [-0.2703, -0.0875],\n",
       "         [-0.7301,  0.3192],\n",
       "         [-0.5973,  0.2568],\n",
       "         [-1.7515,  1.3813]],\n",
       "\n",
       "        [[-1.3691,  1.0970],\n",
       "         [ 0.9623, -2.0420],\n",
       "         [-1.3665,  0.6522],\n",
       "         [-1.4532,  1.4253],\n",
       "         [-0.5782, -0.6397],\n",
       "         [-1.2780,  0.7192],\n",
       "         [-0.3230, -0.3896],\n",
       "         [-0.6876, -0.3466],\n",
       "         [-1.3436,  1.2218],\n",
       "         [-1.3386,  1.0405],\n",
       "         [-0.3157, -0.2422]]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attention(dummy_inputs)['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention.w_q.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "So we have it, a multi-headed attention mechanims from scratch!! However, as you might have already noticed, it only allows attentions between input tokens feeded into the encoder, but not between encoder and decoder tokens. Therefore, this is a **Self Attehntion Mechanism**. In the next section, we'll build other parts of the encoder before moving to the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test for `MultiHeadAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit.multiheaded_attentions import MultiHeadAttention\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def multihead_attention_unit_test():\n",
    "    multihead_attention = MultiHeadAttention(512, 8, 64).to('cpu') #[d_model, num_head, d_k]\n",
    "    dummy_inputs = torch.randn((2, 128, 512)).to('cpu') #[batch_size, seq_len, d_model]\n",
    "    y = multihead_attention(dummy_inputs)\n",
    "    loss = y.mean()\n",
    "    loss.backward()\n",
    "    for name, param in multihead_attention.named_parameters():\n",
    "        assert param.grad is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention_unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afa5cafc83a44ddc6a4432c58e27e1e59e90bf4b5432b3ef6f80dec7803e93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
