{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention mechanism and multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having a encoder-decoder framewark in mind, we now build the compoments of a transformer one by one, starting from the attention mechanism (the selected area below). \n",
    "\n",
    "We'll first build a single-head attention attention, then extend it to multi-head and implement masking for decoder inputs\n",
    "\n",
    "![transformer model with its attention mechanism selected](./photos/Screenshot%20from%202022-09-07%2017-36-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [transformer paper](https://arxiv.org/pdf/1706.03762.pdf), the hidden states are first **linearly projected** to the key vector, query vector, and the value vector, then computed according to the formula below:\n",
    "$$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}V)$$\n",
    "\n",
    "![An illustration of the attention mechanism](./photos/Screenshot%20from%202022-09-07%2018-12-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated in the illustration above, the query vector $Q$ and the key vector $K$ has the same dimensions, that is, $d_q = d_k$. The value vector can have any dimensions $d_v$ if we add an addition linear layer before the $SoftMax$ layer of $Q$ and $K$, yet in this paper $d_k = d_q = d_v$.\n",
    "\n",
    "The linear projection matrices for a single-head attention mechanism, $W^Q$, $W^K$, $W^V$,  are of dimensions $d_{model}\\times d_k$, $d_{model}\\times d_q$, $d_{model}\\times d_v$, respectively.\n",
    "\n",
    "Below is an implementation of the single-head attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_k)\n",
    "        self.w_k = nn.Linear(d_model, d_k)\n",
    "        self.w_v = nn.Linear(d_model, d_k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # q,k,v: [batch_size, seq_len, d_k]\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        # scores, normalized_scores, softmax_scores: [batch_size, seq_len, seq_len]\n",
    "        scores = torch.matmul(q, torch.permute(k, (0, 2, 1)))\n",
    "        normalized_scores = scores/self.d_k\n",
    "        softmax_scores = nn.functional.softmax(normalized_scores, dim=-1)\n",
    "        #TODO: verify whether the softmax is applied along the correct dimesion\n",
    "\n",
    "        # attentions: [batch_size, seq_len, d_k]\n",
    "        attentions = torch.matmul(normalized_scores, v)\n",
    "        return attentions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's write tests to make sure the dimensions and the computed attentions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_head_attention = SingleHeadAttention(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = torch.randn((1,4,5)) # A dummy input of shape [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_outputs = single_head_attention(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dummy_outputs.shape == (1,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7294,  0.2412,  0.0146],\n",
       "         [-0.0643, -0.3173, -0.3077],\n",
       "         [ 1.5143, -0.2022,  0.2589],\n",
       "         [-0.9435, -0.0336, -0.3060]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afa5cafc83a44ddc6a4432c58e27e1e59e90bf4b5432b3ef6f80dec7803e93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
